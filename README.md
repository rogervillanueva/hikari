# Hikari Reader

Hikari Reader is a Next.js Progressive Web App for sentence-aligned bilingual reading between Japanese and English. V1 focuses on client-first storage, mock language services, and an installable PWA scaffold so teams can plug in production-grade providers later without refactoring.

## Overview

* Import long-form Japanese or English text (PDF pipeline stubbed) into IndexedDB for offline-first reading.
* Sentence-level navigation with per-sentence playback, inline translations, and word popups powered by provider adapters.
* Mock translation, dictionary, and TTS providers showcase the adapter pattern and allow the demo to run without paid services.
* Save vocabulary to a built-in SRS deck and practice with an SM-2-inspired scheduler.
* Installable PWA skeleton with a vanilla service worker ready to swap for Workbox.

## Quick Start

```bash
pnpm install
pnpm dev # starts apps/web on http://localhost:3000
```

> Need text-to-speech? Copy `apps/web/.env.local.example` to `apps/web/.env.local`, add your Azure Speech key, then restart the dev server.

1. Visit `http://localhost:3000/dev/seed` and click **Seed Demo Data**.
2. Open `/documents` to browse imported documents.
3. Enter a document to test sentence playback, inline translations, and word popups.
4. Explore `/practice` for the SM-2 flashcard loop.

## Feature Flags & Providers

Provider adapters live under `apps/web/providers/*`.

* **Translation** — `TranslationProvider` interface with a mock implementation returning demo fixture text. Use `getTranslationProvider(id)` to fetch the active provider.
* **Dictionary** — `DictionaryProvider` interface returning mock readings + translation fallback. Replace with JMdict or a licensed API.
* **TTS** — `TtsProvider` interface with a mock beep generator and an Azure Cognitive Services integration. Switch providers with `NEXT_PUBLIC_PROVIDER`.

Set `NEXT_PUBLIC_PROVIDER=mock` (default) to use local beeps, or `NEXT_PUBLIC_PROVIDER=azure` once credentials are configured. Future integrations can branch on this environment variable.

## Environment Variables

The demo does not require keys, but placeholders are documented for future integrations:

* `AZURE_SPEECH_KEY`, `AZURE_SPEECH_REGION`, optional `AZURE_SPEECH_ENDPOINT`
* `AZURE_TTS_BUDGET_CENTS` (default `$1.00` per day)
* `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`
* `OPENAI_API_KEY`, `DEEPL_API_KEY`
* `OCR_SERVER_URL`
* `NEXT_PUBLIC_PROVIDER`
* `NEXT_PUBLIC_TTS_VOICE`

## What’s Stubbed

* PDF text extraction and OCR routes respond with metadata only.
* Mock translation/dictionary/TTS services log requests and return deterministic placeholder data.
* Service worker caches via a simple runtime cache (replace with Workbox for production policies).

## Legal Notes

* Ensure users have rights to the text or PDFs they import; the app stores copies locally.
* Dictionary and pitch-accent resources may require additional licensing before bundling.
* Audio generated by third-party TTS providers must respect their usage terms.

## Performance Tips

* Tokenization and PDF work should happen in Web Workers—current stubs highlight extension points in `apps/web/workers`.
* Use translation windowing (see `providers/translation/mock.ts`) to prefetch only the current viewport.
* IndexedDB tables are keyed for quick lookups by document and sentence ID; prune caches from `/dev/debug` when testing large files.

## Troubleshooting

* **Tokenizer fails** — ensure the mock seed succeeded; kuromoji integration will live in `workers/tokenize-ja.ts`.
* **PDF/OCR fails** — current API routes are placeholders. Implement server-side `pdf-parse` + Tesseract per the README TODO.
* **Large docs feel slow** — adjust page word budget and prefetch windows in `/settings`.

## PWA Setup

* Manifest lives in `apps/web/public/manifest.webmanifest` (Codex text-only mode uses an SVG icon placeholder).
* Service worker logic is in `apps/web/lib/sw.ts`. Replace with Workbox to add background sync, offline routing, and cache quotas.
* Settings include `offlineMode`, `maxAudioCacheMB`, and `maxTranslationCacheMB` to enforce LRU policies once implemented.

## TO-DO (Post-Key Integration Guide)

1. **TTS (Azure/AWS)** — integrate paid providers with timestamp support, store audio blobs in IndexedDB, respect quota.
2. **Translation (DeepL/OpenAI/etc.)** — enforce per-document budget caps and throttle requests.
3. **Dictionary (JMdict/licensed)** — ingest dictionary data and expose toggles in Settings.
4. **Pitch Accent** — add provider implementation returning `{ pattern, accents }` to enrich the word popup.
5. **Server OCR** — wire `/api/pdf/ocr` to a Tesseract CLI container (see docker-compose stub).

## Roadmap

* Multi-user sync and optional cloud backup.
* Enhanced mobile controls and gesture navigation.
* Expanded language support with pluggable tokenizers and RTL layout flags.
* Automated Playwright coverage for import → read → practice flow.

## Repository Structure

```
apps/
  web/
    app/                # Next.js App Router routes (documents, practice, settings, dev tools)
    components/         # Reader UI, import forms, shared components
    fixtures/           # Demo Japanese/English text pairs
    lib/                # Dexie schema, SRS helpers, service worker
    providers/          # Adapter interfaces + mock implementations
    scripts/            # Dev utilities (seed demo)
    store/              # Zustand stores for documents, settings, SRS
    workers/            # Sentence splitter placeholder (expand with tokenizers/OCR)
```

## Testing

* Unit testing via Vitest (scaffolded). Add suites for translation windowing and SRS calculations.
* Playwright planned for E2E flows: import, reader playback, word popup, SRS review.

## Docker & OCR Server

* Stub files under `apps/ocr-server` should define a minimal Express + Tesseract pipeline (to be implemented).
* `docker-compose.yml` will orchestrate the OCR container alongside the Next.js dev server in future iterations.

## Contributing

1. Fork the repo and clone locally.
2. Create a feature branch following `feature/...` naming.
3. Run `pnpm lint && pnpm test` before submitting a PR.
4. Document any new providers or settings.

---

### TO-DO: Production Integrations

* **TTS (Azure Cognitive Services)**

  * Install: `@azure/cognitiveservices-speech-sdk`
  * Env: `AZURE_SPEECH_KEY`, `AZURE_SPEECH_REGION`
  * Use SSML `<mark name="s{index}"/>` per sentence; capture `viseme/BookmarkReached` events → map to sentence indices.
  * Store audio to IndexedDB `audio` table; key = hash(provider, voice, text).
* **TTS (AWS Polly)**

  * Env: `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`
  * Use SpeechMarks to derive per-word timings; aggregate to sentence timings.
* **Translation (DeepL/OpenAI/Router)**

  * Implement `estimateCost(chars)` using provider pricing tables; block if over `budgetCents`.
  * Translate in batches aligned to **sentences**. Persist by `sentenceId`.
* **Dictionary (JMdict or Licensed)**

  * Script to import XML/JSON → IndexedDB table `jmdict_entries` (optional). Map to `Definition[]`.
  * Respect license terms; provide toggle in Settings.
* **Pitch Accent**

  * Expected schema for `PitchInfo`: `{ pattern:string; accents:number[] }`.
  * UI shows Tokyo pattern lines above reading when available.
* **OCR Server**

  * Build: `docker compose up -d ocr`
  * Env for web app: `OCR_SERVER_URL` → enables `/api/pdf/ocr` proxy.

### Demo Mode

* `NEXT_PUBLIC_PROVIDER=mock` enables mock translation/TTS/dictionary.
* Visit `/dev/seed` → **Seed Demo Data** → open your document and test.

### PWA Notes

* First load while online; then offline use for saved docs + SRS.
* Manage cache sizes in Settings; use **Clear caches** if storage quota prompts.
